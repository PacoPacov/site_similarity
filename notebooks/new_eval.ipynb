{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitsitesimilarityconda5cc228f1d50144ce9681545e76d7f6e7",
   "display_name": "Python 3.6.10 64-bit ('site_similarity': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(\"/home/paco/Documents/site_similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Using model: node2vec_unweighted_3_levels_128D.model\n"
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/paco/Documents/site_similarity/dataset/annotated_data/modified_corpus_2018.csv'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1bfca15e0c11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mCs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ovr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     )\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode2vec_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode2vec_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_year\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2018'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/site_similarity/utils/notebook_utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(clf, node2vec_model, data_year, num_labels)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mSPLITS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'modified_splits_new_corpus_2020.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdata_year\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'2018'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mDATA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'modified_corpus_2018.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mLOADED_SPLITS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'splits_2018.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/site_similarity/utils/notebook_utils.py\u001b[0m in \u001b[0;36mload_corpus\u001b[0;34m(corpus_file)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ANNOTATED_DATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0mcsv_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/paco/Documents/site_similarity/dataset/annotated_data/modified_corpus_2018.csv'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from utils.notebook_utils import train_model\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "models = [\n",
    "    'node2vec_unweighted_3_levels_128D.model',\n",
    "    'node2vec_unweighted_3_levels_64D.model',\n",
    "    'node2vec_weighted_3_levels_128D.model',\n",
    "    'node2vec_weighted_3_levels_64D.model'\n",
    "]\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    print(f'Using model: {model}')\n",
    "    node2vec_model = Word2Vec.load(model)\n",
    "\n",
    "    clf = LogisticRegressionCV(\n",
    "        Cs=10, cv=5, scoring=\"accuracy\", verbose=True, multi_class=\"ovr\", max_iter=300, random_state=42\n",
    "    )\n",
    "    train_model(clf, node2vec_model=node2vec_model, data_year='2018')\n",
    "\n",
    "\n",
    "    clf2 = LogisticRegressionCV(\n",
    "        Cs=10, cv=10, scoring=\"accuracy\", verbose=True, multi_class=\"ovr\", max_iter=300, random_state=42\n",
    "    )\n",
    "    train_model(clf2, node2vec_model=node2vec_model, data_year='2018')\n",
    "\n",
    "    tree_clf = GradientBoostingClassifier(random_state=42)\n",
    "    train_model(tree_clf, node2vec_model=node2vec_model, data_year='2018')\n",
    "\n",
    "    svm_clf = svm.SVC(decision_function_shape='ovo', probability=True) \n",
    "    train_model(svm_clf, node2vec_model=node2vec_model, data_year='2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}